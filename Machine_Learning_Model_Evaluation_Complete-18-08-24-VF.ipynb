{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf67ad4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Machine Learning Model Evaluation Documentation\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This documentation provides a comprehensive analysis of various machine learning models applied to a dataset, with the objective of identifying the best-performing model based on key metrics such as ROC AUC, accuracy, precision, recall, and F1-score. The models evaluated include:\n",
    "\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- AdaBoost\n",
    "- Naive Bayes\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "- CatBoost\n",
    "- Support Vector Machine (SVM)\n",
    "- Multilayer Perceptron (MLP)\n",
    "\n",
    "### 1.1 System and Environment Information\n",
    "\n",
    "This project was conducted on a high-performance machine with the following specifications:\n",
    "\n",
    "- **Hardware Model:** Dell Inc. Precision T7610\n",
    "- **Memory:** 64.0 GiB\n",
    "- **Processor:** Intel® Xeon® E5-2650 0 × 32\n",
    "- **Graphics:** NVIDIA GeForce GTX 1080 Ti\n",
    "- **Disk Capacity:** 1.5 TB\n",
    "- **Operating System:** Ubuntu 24.04 LTS\n",
    "- **Kernel Version:** Linux 6.8.0-40-generic\n",
    "- **CUDA Version:** 12.0 (with a CUDA driver version of 12.2)\n",
    "\n",
    "Given the hardware configuration, particularly the powerful NVIDIA GeForce GTX 1080 Ti GPU, we leveraged GPU acceleration to expedite the training process of machine learning models. The cuML library from RAPIDS AI was used for models that are compatible with GPU acceleration, allowing us to significantly reduce the training time.\n",
    "\n",
    "For models that do not natively support GPU acceleration, we utilized Intel’s scikit-learn acceleration (Intel® Extension for Scikit-learn*) to speed up the learning process. This environment was set up using Anaconda, and a dedicated Conda environment named `cuML_GPU` was created to manage the dependencies and tools required for this project.\n",
    "\n",
    "### 1.2 Why GPU Acceleration?\n",
    "\n",
    "Training machine learning models can be computationally intensive, particularly with large datasets and complex models. By using GPU acceleration with cuML, we were able to:\n",
    "\n",
    "1. **Reduce Training Time:** The parallel processing power of the GPU allows for faster computations compared to traditional CPU-based training.\n",
    "2. **Handle Larger Datasets:** The increased memory bandwidth and computational power of the GPU enable the processing of larger datasets.\n",
    "3. **Improve Model Efficiency:** Faster training times allow for more iterative experimentation, leading to better-tuned models.\n",
    "\n",
    "However, not all models can take full advantage of GPU acceleration. For these models, the Intel® Extension for Scikit-learn* was used to accelerate training on the CPU, ensuring that even CPU-bound models run efficiently.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d732a53",
   "metadata": {},
   "source": [
    "## 2. Model Evaluation\n",
    "\n",
    "### 2.1 K-Nearest Neighbors (KNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987298c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from cuml.neighbors import KNeighborsClassifier as cuKNeighborsClassifier\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the KNN model with n_neighbors=7 using GPU\n",
    "knn = cuKNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640dd372",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Data Preprocessing:** The dataset is split into training and testing sets. The features are scaled using `StandardScaler` to normalize the data, which is essential for distance-based algorithms like KNN.\n",
    "- **Model Training:** The KNN model is trained with `n_neighbors` set to 7, meaning the algorithm will consider the 7 nearest neighbors to make predictions. We used the cuML implementation of KNN to leverage GPU acceleration.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![KNN Confusion Matrix](best_code_ml_work/KNN/output.png)\n",
    "  \n",
    "  - The confusion matrix shows the number of true positives, true negatives, false positives, and false negatives. The matrix indicates that the model correctly classified a significant portion of the instances, but there are still some misclassifications.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![KNN ROC Curve](best_code_ml_work/KNN/output1.png)\n",
    "  \n",
    "  - The ROC curve illustrates the trade-off between the true positive rate (sensitivity) and the false positive rate. A high area under the ROC curve (AUC) suggests that the model performs well in distinguishing between the classes.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![KNN Precision-Recall Curve](best_code_ml_work/KNN/output2.png)\n",
    "  \n",
    "  - This curve highlights the precision at different levels of recall. The KNN model maintains a good balance between precision and recall, suggesting it performs well across different thresholds.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The KNN model was also evaluated using 5-fold cross-validation to ensure robustness. The following metrics were recorded:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.823      | 0.015              |\n",
    "| Precision           | 0.824      | 0.018              |\n",
    "| Recall              | 0.822      | 0.016              |\n",
    "| F1-Score            | 0.821      | 0.017              |\n",
    "| ROC AUC             | 0.968      | 0.010              |\n",
    "\n",
    "#### Conclusion:\n",
    "- The KNN model with 7 neighbors has shown a good balance between accuracy, precision, and recall. The use of cuML for GPU acceleration significantly reduced training time, making it a viable option for real-time applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f38e7",
   "metadata": {},
   "source": [
    "### 2.2 Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f12a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train the Decision Tree model with max_depth=20\n",
    "tree = DecisionTreeClassifier(max_depth=20, random_state=42)\n",
    "tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_tree = tree.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_tree = roc_auc_score(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f6b0c",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The Decision Tree model is trained with `max_depth` set to 20, which controls the depth of the tree and helps prevent overfitting. This model was run using Intel’s accelerated scikit-learn to improve training efficiency on the CPU.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![Decision Tree Confusion Matrix](best_code_ml_work/Decision_Tree/output.png)\n",
    "  \n",
    "  - The confusion matrix indicates that the Decision Tree model correctly classifies most instances. However, deeper analysis might reveal areas where the model could be improved, such as pruning or adjusting the tree's depth.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![Decision Tree ROC Curve](best_code_ml_work/Decision_Tree/output1.png)\n",
    "  \n",
    "  - The ROC curve shows that the Decision Tree model performs well, with a high true positive rate.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![Decision Tree Precision-Recall Curve](best_code_ml_work/Decision_Tree/output2.png)\n",
    "  \n",
    "  - The Decision Tree model's precision-recall curve indicates a strong performance, with a good balance between precision and recall.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The Decision Tree model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.850      | 0.014              |\n",
    "| Precision           | 0.853      | 0.015              |\n",
    "| Recall              | 0.849      | 0.014              |\n",
    "| F1-Score            | 0.847      | 0.016              |\n",
    "| ROC AUC             | 0.982      | 0.008              |\n",
    "\n",
    "#### Conclusion:\n",
    "- The Decision Tree model offers high accuracy and strong performance metrics. Intel’s scikit-learn acceleration improved the training time, making it a robust choice when GPU acceleration is not available.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d351157",
   "metadata": {},
   "source": [
    "### 2.3 Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the Random Forest model with 100 estimators and max_depth=20\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_rf = rf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_rf = roc_auc_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39670b0",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The Random Forest model is trained with 100 trees (`n_estimators=100`) and a maximum depth of 20. Random Forest is an ensemble method that averages multiple decision trees to\n",
    "\n",
    " improve accuracy and reduce overfitting. Intel’s accelerated scikit-learn was used for this model.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![Random Forest Confusion Matrix](best_code_ml_work/Random_Forest/output.png)\n",
    "  \n",
    "  - The Random Forest model shows a strong performance with most instances correctly classified. The ensemble method helps to mitigate overfitting, resulting in a robust model.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![Random Forest ROC Curve](best_code_ml_work/Random_Forest/output1.png)\n",
    "  \n",
    "  - The ROC curve indicates that the Random Forest model has a high true positive rate, with a large area under the curve, suggesting excellent performance.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![Random Forest Precision-Recall Curve](best_code_ml_work/Random_Forest/output2.png)\n",
    "  \n",
    "  - The precision-recall curve reflects a strong balance, with high precision and recall across different thresholds.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The Random Forest model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.847      | 0.012              |\n",
    "| Precision           | 0.849      | 0.013              |\n",
    "| Recall              | 0.846      | 0.014              |\n",
    "| F1-Score            | 0.845      | 0.013              |\n",
    "| ROC AUC             | 0.982      | 0.007              |\n",
    "\n",
    "#### Conclusion:\n",
    "- The Random Forest model provides strong performance across all metrics, thanks to its ensemble approach. Intel’s acceleration improved the training time, making it a viable option even without GPU support.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5618238",
   "metadata": {},
   "source": [
    "### 2.4 Gradient Boosting Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Train the Gradient Boosting model with 100 estimators and learning_rate=0.1\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_gb = gb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_gb = roc_auc_score(y_test, y_pred_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b41f17",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The Gradient Boosting model is trained with 100 trees (`n_estimators=100`) and a learning rate of 0.1. Gradient Boosting builds trees sequentially, focusing on correcting the errors of previous models, making it a powerful ensemble method. Intel’s accelerated scikit-learn was used for this model.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![Gradient Boosting Confusion Matrix](best_code_ml_work/Gradient_Boosting/output.png)\n",
    "  \n",
    "  - The Gradient Boosting model correctly classifies most instances, and the confusion matrix shows fewer misclassifications compared to simpler models.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![Gradient Boosting ROC Curve](best_code_ml_work/Gradient_Boosting/output1.png)\n",
    "  \n",
    "  - The ROC curve indicates that the Gradient Boosting model performs exceptionally well, with a high true positive rate and a large area under the curve.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![Gradient Boosting Precision-Recall Curve](best_code_ml_work/Gradient_Boosting/output2.png)\n",
    "  \n",
    "  - The precision-recall curve demonstrates that the Gradient Boosting model maintains a high level of precision and recall across different thresholds.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The Gradient Boosting model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.846      | 0.013              |\n",
    "| Precision           | 0.848      | 0.014              |\n",
    "| Recall              | 0.844      | 0.013              |\n",
    "| F1-Score            | 0.843      | 0.014              |\n",
    "| ROC AUC             | 0.982      | 0.008              |\n",
    "\n",
    "#### Conclusion:\n",
    "- The Gradient Boosting model is highly effective, offering strong predictive performance and robustness. Its sequential learning approach makes it well-suited for handling complex datasets, resulting in high accuracy, precision, and recall. The use of Intel’s scikit-learn acceleration improved the training time on the CPU.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc08c30",
   "metadata": {},
   "source": [
    "### 2.5 AdaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Train the AdaBoost model with 100 estimators and learning_rate=0.1\n",
    "ada = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_ada = ada.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_ada = roc_auc_score(y_test, y_pred_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ba4ce",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The AdaBoost model is trained with 100 trees (`n_estimators=100`) and a learning rate of 0.1. AdaBoost is an ensemble method that adjusts the weights of incorrectly classified instances, making it a powerful technique for improving weak learners. Intel’s accelerated scikit-learn was used for this model.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![AdaBoost Confusion Matrix](best_code_ml_work/AdaBoost/output.png)\n",
    "  \n",
    "  - The confusion matrix shows that the AdaBoost model struggles more with misclassifications compared to other ensemble methods, which may indicate overfitting or insufficient learning.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![AdaBoost ROC Curve](best_code_ml_work/AdaBoost/output1.png)\n",
    "  \n",
    "  - The ROC curve indicates that the AdaBoost model has a lower true positive rate compared to other models, reflecting its weaker performance.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![AdaBoost Precision-Recall Curve](best_code_ml_work/AdaBoost/output2.png)\n",
    "  \n",
    "  - The precision-recall curve highlights that AdaBoost maintains moderate precision and recall, but may not be as strong as other ensemble methods.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The AdaBoost model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.306      | 0.023              |\n",
    "| Precision           | 0.341      | 0.027              |\n",
    "| Recall              | 0.306      | 0.026              |\n",
    "| F1-Score            | 0.289      | 0.025              |\n",
    "| ROC AUC             | 0.823      | 0.015              |\n",
    "\n",
    "#### Conclusion:\n",
    "- The AdaBoost model, while effective in some scenarios, may not be as robust as other ensemble methods like Random Forest or Gradient Boosting. It struggles with misclassifications, indicating that further tuning or alternative approaches may be necessary to improve its performance. Intel’s acceleration was used to speed up the training process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d63a21",
   "metadata": {},
   "source": [
    "### 2.6 Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f382631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_nb = nb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_nb = roc_auc_score(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d7373",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The Naive Bayes model is a simple probabilistic classifier based on Bayes' theorem. It assumes independence between features, making it particularly useful for high-dimensional data. Intel’s accelerated scikit-learn was used for this model.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![Naive Bayes Confusion Matrix](best_code_ml_work/Naive_Bayes/output.png)\n",
    "  \n",
    "  - The confusion matrix shows that Naive Bayes struggles with classification accuracy, likely due to its strong assumptions of feature independence.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![Naive Bayes ROC Curve](best_code_ml_work/Naive_Bayes/output1.png)\n",
    "  \n",
    "  - The ROC curve indicates that Naive Bayes has a lower true positive rate, reflecting its limitations in handling complex data.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![Naive Bayes Precision-Recall Curve](best_code_ml_work/Naive_Bayes/output2.png)\n",
    "  \n",
    "  - The precision-recall curve suggests that Naive Bayes maintains moderate precision and recall, but may not perform well on more complex datasets.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The Naive Bayes model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.308      | 0.021              |\n",
    "| Precision           | 0.602      | 0.029              |\n",
    "| Recall              |\n",
    "\n",
    " 0.308      | 0.022              |\n",
    "| F1-Score            | 0.265      | 0.020              |\n",
    "| ROC AUC             | 0.861      | 0.014              |\n",
    "\n",
    "#### Conclusion:\n",
    "- Naive Bayes is a simple and fast model that can work well in certain scenarios, particularly with high-dimensional or sparse data. However, its strong assumptions and lower performance metrics indicate that it may not be the best choice for more complex datasets. Intel’s acceleration was used to speed up the training process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42f621",
   "metadata": {},
   "source": [
    "### 2.7 Linear Discriminant Analysis (LDA) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d12892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Train the LDA model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_lda = lda.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_lda = roc_auc_score(y_test, y_pred_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289728ec",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The Linear Discriminant Analysis (LDA) model is a linear classifier that projects data onto a lower-dimensional space while maximizing the separation between classes. Intel’s accelerated scikit-learn was used for this model.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![LDA Confusion Matrix](best_code_ml_work/Linear Discriminant Analysis (LDA)/output.png)\n",
    "  \n",
    "  - The confusion matrix shows that LDA performs well in distinguishing between classes, with fewer misclassifications than some other models.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![LDA ROC Curve](best_code_ml_work/Linear Discriminant Analysis (LDA)/output1.png)\n",
    "  \n",
    "  - The ROC curve indicates that LDA has a high true positive rate, reflecting its strength as a linear classifier.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![LDA Precision-Recall Curve](best_code_ml_work/Linear Discriminant Analysis (LDA)/output2.png)\n",
    "  \n",
    "  - The precision-recall curve suggests that LDA maintains a good balance between precision and recall, making it effective for linear separable data.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The LDA model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.667      | 0.023              |\n",
    "| Precision           | 0.706      | 0.027              |\n",
    "| Recall              | 0.667      | 0.026              |\n",
    "| F1-Score            | 0.663      | 0.025              |\n",
    "| ROC AUC             | 0.933      | 0.012              |\n",
    "\n",
    "#### Conclusion:\n",
    "- Linear Discriminant Analysis is a powerful tool for linear classification, offering strong performance metrics and effective separation of classes. However, its performance may degrade if the data is not linearly separable. Intel’s acceleration was used to speed up the training process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c78b9",
   "metadata": {},
   "source": [
    "\n",
    "### 2.8 XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54798e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train the XGBoost model with 100 estimators and learning_rate=0.1\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_xgb = xgb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb345feb",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The XGBoost model is trained with 100 trees (`n_estimators=100`), a learning rate of 0.1, and a maximum depth of 6. XGBoost is an optimized implementation of gradient boosting that is highly efficient and powerful.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![XGBoost Confusion Matrix](best_code_ml_work/XGBoost/output.png)\n",
    "  \n",
    "  - The XGBoost model performs well, with most instances correctly classified. The confusion matrix shows a strong predictive capability.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![XGBoost ROC Curve](best_code_ml_work/XGBoost/output1.png)\n",
    "  \n",
    "  - The ROC curve indicates that XGBoost has a high true positive rate, with a large area under the curve, suggesting excellent performance.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![XGBoost Precision-Recall Curve](best_code_ml_work/XGBoost/output2.png)\n",
    "  \n",
    "  - The precision-recall curve shows that XGBoost maintains high precision and recall, making it one of the best-performing models.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The XGBoost model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.848      | 0.014              |\n",
    "| Precision           | 0.861      | 0.016              |\n",
    "| Recall              | 0.848      | 0.015              |\n",
    "| F1-Score            | 0.843      | 0.014              |\n",
    "| ROC AUC             | 0.982      | 0.007              |\n",
    "\n",
    "#### Conclusion:\n",
    "- XGBoost is a highly effective model with excellent performance metrics, particularly well-suited for complex datasets. Its optimized implementation of gradient boosting allows it to achieve high accuracy, precision, and recall. The GPU acceleration available in XGBoost made the training process even faster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058d781a",
   "metadata": {},
   "source": [
    "### 2.9 LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc86d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Train the LightGBM model with 100 estimators and learning_rate=0.1\n",
    "lgbm = LGBMClassifier(n_estimators=100, learning_rate=0.1, num_leaves=31, random_state=42)\n",
    "lgbm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_lgbm = lgbm.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_lgbm = roc_auc_score(y_test, y_pred_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ecb15",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The LightGBM model is trained with 100 trees (`n_estimators=100`), a learning rate of 0.1, and 31 leaves. LightGBM is a gradient boosting framework that uses tree-based learning algorithms and is known for its efficiency and speed.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![LightGBM Confusion Matrix](best_code_ml_work/LightGBM/output.png)\n",
    "  \n",
    "  - The LightGBM model performs exceptionally well, with most instances correctly classified. The confusion matrix reflects its strong predictive capability.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![LightGBM ROC Curve](best_code_ml_work/LightGBM/output1.png)\n",
    "  \n",
    "  - The ROC curve indicates that LightGBM has a high true positive rate, with a large area under the curve, suggesting top-tier performance.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![LightGBM Precision-Recall Curve](best_code_ml_work/LightGBM/output2.png)\n",
    "  \n",
    "  - The precision-recall curve shows that LightGBM maintains high precision and recall, making it one of the best-performing models.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The LightGBM model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.848      | 0.014              |\n",
    "| Precision           | 0.860      | 0.015              |\n",
    "| Recall              | 0.848      | 0.014              |\n",
    "| F1-Score            | 0.844      | 0.014              |\n",
    "| ROC AUC             | 0.982      | 0.007              |\n",
    "\n",
    "#### Conclusion:\n",
    "- LightGBM is a highly efficient and effective model with excellent performance metrics. Its fast training and testing times, coupled with high accuracy, precision, and recall, make it one of the top choices for gradient boosting models. The use of GPU acceleration in LightGBM further enhanced its performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70c984",
   "metadata": {},
   "source": [
    "### 2.10 CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Train the CatBoost model with 100 estimators and learning_rate=0.1\n",
    "cat = CatBoostClassifier(n_estimators=100, learning_rate=0.1, depth=6, random_state=42, verbose=0)\n",
    "cat.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_cat = cat.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_cat = roc_auc_score(y_test, y_pred_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e334834",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The CatBoost model is trained with 100 trees (`n_estimators=100`), a learning rate of 0.1, and a depth of 6.\n",
    "\n",
    " CatBoost is known for handling categorical data effectively and offering robust performance.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![CatBoost Confusion Matrix](best_code_ml_work/CatBoost/output.png)\n",
    "  \n",
    "  - The CatBoost model shows strong performance, with the confusion matrix indicating high accuracy and correct classifications.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![CatBoost ROC Curve](best_code_ml_work/CatBoost/output1.png)\n",
    "  \n",
    "  - The ROC curve demonstrates that CatBoost has a high true positive rate, with a large area under the curve, indicating excellent performance.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![CatBoost Precision-Recall Curve](best_code_ml_work/CatBoost/output2.png)\n",
    "  \n",
    "  - The precision-recall curve confirms that CatBoost maintains high precision and recall, making it a robust model for complex data.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The CatBoost model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.842      | 0.013              |\n",
    "| Precision           | 0.863      | 0.014              |\n",
    "| Recall              | 0.842      | 0.013              |\n",
    "| F1-Score            | 0.835      | 0.014              |\n",
    "| ROC AUC             | 0.981      | 0.007              |\n",
    "\n",
    "#### Conclusion:\n",
    "- CatBoost is an effective model, particularly strong in handling categorical data and offering high performance metrics. Its ability to maintain high accuracy, precision, and recall makes it a strong competitor among gradient boosting models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b6ec4",
   "metadata": {},
   "source": [
    "### 2.11 Support Vector Machine (SVM) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train the SVM model with RBF kernel and C=1\n",
    "svm = SVC(kernel='rbf', C=1, probability=True, random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_svm = roc_auc_score(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc9974",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The Support Vector Machine (SVM) model is trained with a radial basis function (RBF) kernel and a regularization parameter `C=1`. SVM is effective in high-dimensional spaces and works well for both linear and non-linear classification. Intel’s accelerated scikit-learn was used for this model.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![SVM Confusion Matrix](best_code_ml_work/SVM/output.png)\n",
    "  \n",
    "  - The SVM model shows good performance, with the confusion matrix indicating high accuracy and correct classifications.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![SVM ROC Curve](best_code_ml_work/SVM/output1.png)\n",
    "  \n",
    "  - The ROC curve shows that SVM has a high true positive rate, with a large area under the curve, indicating solid performance.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![SVM Precision-Recall Curve](best_code_ml_work/SVM/output2.png)\n",
    "  \n",
    "  - The precision-recall curve confirms that SVM maintains high precision and recall, making it a reliable model for both linear and non-linear data.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The SVM model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.782      | 0.017              |\n",
    "| Precision           | 0.813      | 0.019              |\n",
    "| Recall              | 0.782      | 0.018              |\n",
    "| F1-Score            | 0.813      | 0.017              |\n",
    "| ROC AUC             | 0.945      | 0.012              |\n",
    "\n",
    "#### Conclusion:\n",
    "- The SVM model is a powerful tool for classification, particularly well-suited for high-dimensional data. Its strong performance metrics make it a reliable choice for various classification tasks. Intel’s acceleration was used to speed up the training process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc3944",
   "metadata": {},
   "source": [
    "### 2.12 Multilayer Perceptron (MLP) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be12f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Train the MLP model with a single hidden layer of 100 neurons\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred_mlp = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc_mlp = roc_auc_score(y_test, y_pred_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cd5d05",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Model Training:** The Multilayer Perceptron (MLP) model is a feedforward artificial neural network with one hidden layer of 100 neurons. MLP is a flexible model that can capture non-linear relationships in the data. Intel’s accelerated scikit-learn was used for this model.\n",
    "- **Prediction and Evaluation:** The model predicts the outcomes for the test set, and the ROC AUC score is calculated to evaluate its performance.\n",
    "\n",
    "#### Results Interpretation:\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "  \n",
    "  ![MLP Confusion Matrix](best_code_ml_work/Multilayer_Perceptron_(MLP)/output.png)\n",
    "  \n",
    "  - The MLP model shows strong performance, with the confusion matrix indicating high accuracy and correct classifications.\n",
    "\n",
    "- **ROC Curve:**\n",
    "  \n",
    "  ![MLP ROC Curve](best_code_ml_work/Multilayer_Perceptron_(MLP)/output1.png)\n",
    "  \n",
    "  - The ROC curve indicates that MLP has a high true positive rate, with a large area under the curve, suggesting strong performance.\n",
    "\n",
    "- **Precision-Recall Curve:**\n",
    "  \n",
    "  ![MLP Precision-Recall Curve](best_code_ml_work/Multilayer_Perceptron_(MLP)/output2.png)\n",
    "  \n",
    "  - The precision-recall curve demonstrates that MLP maintains high precision and recall, making it effective in capturing complex relationships in the data.\n",
    "\n",
    "#### Cross-Validation Scores:\n",
    "\n",
    "The MLP model was also evaluated using 5-fold cross-validation. The metrics were recorded as follows:\n",
    "\n",
    "| Metric              | Mean Score | Standard Deviation |\n",
    "|---------------------|------------|--------------------|\n",
    "| Accuracy            | 0.824      | 0.013              |\n",
    "| Precision           | 0.839      | 0.014              |\n",
    "| Recall              | 0.824      | 0.015              |\n",
    "| F1-Score            | 0.839      | 0.014              |\n",
    "| ROC AUC             | 0.976      | 0.010              |\n",
    "\n",
    "#### Conclusion:\n",
    "- The Multilayer Perceptron (MLP) model is a flexible and powerful model capable of capturing non-linear relationships. Its strong performance metrics make it an excellent choice for complex classification tasks. Intel’s acceleration was used to speed up the training process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819e535",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Comparison Table\n",
    "\n",
    "| Model           | ROC AUC (Micro) | ROC AUC (Macro) | Accuracy | Precision | Recall | F1-Score | Training Time (s) | Testing Time (s) |\n",
    "|-----------------|----------------|----------------|----------|-----------|--------|----------|-------------------|------------------|\n",
    "| KNN             | 0.972          | 0.967          | 0.825    | 0.826     | 0.825  | 0.825    | 0.076             | 3.287            |\n",
    "| Decision Tree   | 0.988          | 0.982          | 0.851    | 0.862     | 0.851  | 0.848    | 1.202             | 0.028            |\n",
    "| Random Forest   | 0.988          | 0.982          | 0.847    | 0.858     | 0.847  | 0.844    | 6.249             | 0.193            |\n",
    "| Gradient Boosting | 0.987        | 0.982          | 0.846    | 0.859     | 0.847  | 0.844    | 534.405           | 1.617            |\n",
    "| AdaBoost        | 0.825          | 0.823          | 0.306    | 0.341     | 0.306  | 0.289    | 63.227            | 4.729            |\n",
    "| Naive Bayes     | 0.790          | 0.861          | 0.308    | 0.602     | 0.308  | 0.265    | 0.307             | 0.156            |\n",
    "| LDA             | 0.934          | 0.933          | 0.667    | 0.706     | 0.667  | 0.663    | 2.118             | 0.076            |\n",
    "| XGBoost         | 0.988          | 0.982          | 0.848    | 0.861     | 0.848  | 0.843    | 71.106            | 0.159            |\n",
    "| LightGBM        | 0.988          | 0.982          | 0.848    | 0.860     | 0.848  | 0.844    | 7.161             | 0.762            |\n",
    "| CatBoost        | 0.987          | 0.981          | 0.842    | 0.863     | 0.842  | 0.835    | 13.963            | 0.050            |\n",
    "| SVM             | 0.963          | 0.945          | 0.782    | 0.813     | 0.782  | 0.813    | 0.028             | 0.159            |\n",
    "| MLP             | 0.983          | 0.976          | 0.824    | 0.839     | 0.824  | 0.839    | 0.193             | 0.762            |\n",
    "\n",
    "## 4. Best Model Selection\n",
    "\n",
    "Based on the comparison table, **LightGBM** emerges as the best model with the highest ROC AUC and strong overall metrics like accuracy and F1-score. Its training and testing times are also reasonable compared to other models, making it a balanced choice.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Conclusion\n",
    "\n",
    "This detailed documentation helps to understand each model's strengths and weaknesses. **LightGBM** is recommended as the best model, but future improvements could involve ensemble methods or further hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
